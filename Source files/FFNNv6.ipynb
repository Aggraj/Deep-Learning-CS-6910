{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFNNv6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJHqabUBIYwJqoIshhnVFS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aggraj/Deep-Learning-CS-6910/blob/main/FFNNv6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stUyDH6Tw9D2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Feedforwardneuralnetwork:\n",
        "\n",
        "    def __init__(self,n_inputs,n_hidden,n_outputs):\n",
        "\n",
        "        self.n_inputs   = n_inputs\n",
        "        self.n_outputs  = n_outputs\n",
        "        self.n_hidden   = n_hidden\n",
        "        self.weights    = []\n",
        "        self.biases     = []\n",
        "\n",
        "        layers = [self.n_inputs] + self.n_hidden + [self.n_outputs]\n",
        "        for i in range(len(n_hidden)+1):\n",
        "            self.weights.append(np.random.randn(layers[i+1],layers[i]))\n",
        "            self.biases.append(np.random.randn(layers[i+1],1))\n",
        "\n",
        "\n",
        "\n",
        "    def sigmoid(self,x):\n",
        "        return 1 / ( 1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self,x):\n",
        "        soft = np.zeros(x.shape)\n",
        "        for i in range(0, x.shape[1]):\n",
        "            numr = np.exp(x[:, i])\n",
        "            soft[:, i] = numr/np.sum(numr)\n",
        "        return soft\n",
        "\n",
        "    def forward_propagation(self,input):\n",
        "\n",
        "        self.intermidiate_inputs = []\n",
        "        self.post_outputs  = []\n",
        "        W      = self.weights\n",
        "        b      = self.biases\n",
        "\n",
        "        k=0\n",
        "        self.intermidiate_inputs.append(np.matmul(W[k],input)+b[k])\n",
        "        self.post_outputs.append(self.sigmoid(self.intermidiate_inputs[k]))\n",
        "\n",
        "        for k in range(1,len(self.n_hidden)):\n",
        "            self.intermidiate_inputs.append(np.matmul(W[k],self.post_outputs[k-1])+b[k])\n",
        "            self.post_outputs.append(self.sigmoid(self.intermidiate_inputs[k]))\n",
        "\n",
        "        k=len(self.n_hidden)\n",
        "        self.intermidiate_inputs.append(np.matmul(W[k],self.post_outputs[k-1])+b[k])\n",
        "        self.post_outputs.append(self.softmax(self.intermidiate_inputs[k]))\n",
        "\n",
        "        return self.post_outputs[-1]\n",
        "\n",
        "    def back_propagation(self,train_images,train_labels):\n",
        "\n",
        "        g_weights = [0]*(len(self.weights))\n",
        "        g_biases  = [0]*(len(self.biases))\n",
        "        g_a       = [0]*(len(self.n_hidden)+1)\n",
        "        g_h       = [0]*(len(self.n_hidden)+1)\n",
        "        n_samples = train_images.shape[0]  # Change depending on the dimensions of data\n",
        "\n",
        "\n",
        "        for k in reversed(range(len(self.n_hidden)+1)):\n",
        "            if k == len(self.n_hidden):\n",
        "                g_a[k] = self.post_outputs[k] - train_labels  # keep or remove T depending on the dimensions of data\n",
        "                #g_weights[k] = (1/n_samples)*np.matmul(g_a[k],self.post_outputs[k-1].T)\n",
        "                #g_biases[k]  = (1/n_samples)*np.sum(g_a[k], axis=1, keepdims = True)\n",
        "            else:\n",
        "                g_h[k] = (1/n_samples)*np.matmul(self.weights[k+1].T,g_a[k+1])\n",
        "                g_a[k] = (1/n_samples)*np.multiply(g_h[k],np.multiply(self.sigmoid(self.intermidiate_inputs[k]),(1-self.sigmoid(self.intermidiate_inputs[k]))))\n",
        "\n",
        "            if k == 0:\n",
        "                g_weights[k] = (1/n_samples)*np.matmul(g_a[k],train_images.T)\n",
        "            else:\n",
        "                g_weights[k] = (1/n_samples)*np.matmul(g_a[k],self.post_outputs[k-1].T)\n",
        "\n",
        "            g_biases[k]  = (1/n_samples)*np.sum(g_a[k], axis=1, keepdims = True)\n",
        "        return g_weights,g_biases\n",
        "\n",
        "\n",
        "\n",
        "    def train_model(self,train_images,train_labels,epochs,learning_rate,opt='gd',gamma = 0.9,beta = 0.99): \n",
        "         \n",
        "        pre_delta_w = np.multiply(self.weights,0)\n",
        "        pre_delta_b = np.multiply(self.biases,0)\n",
        "        vw = 0\n",
        "        vb = 0\n",
        "        eps = 1e-8\n",
        "        lr_w = 0\n",
        "        lr_b = 0\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(epochs+1):\n",
        "            output =  self.forward_propagation(train_images)\n",
        "            g_weights,g_biases = self.back_propagation(train_images,train_labels)\n",
        "            if opt == 'gd':\n",
        "                delta_w = np.multiply(learning_rate,g_weights)\n",
        "                delta_b = np.multiply(learning_rate,g_biases)\n",
        "          \n",
        "            if opt == 'mgd':\n",
        "                 delta_w = np.multiply(gamma,pre_delta_w) + np.multiply(learning_rate,g_weights)\n",
        "                 delta_b = np.multiply(gamma,pre_delta_b) + np.multiply(learning_rate,g_biases)\n",
        "                 pre_delta_w = delta_w\n",
        "                 pre_delta_b = delta_b\n",
        "\n",
        "            if opt == 'ngd':\n",
        "\n",
        "                self.weights = self.weights - np.multiply(gamma,pre_delta_w)\n",
        "                self.biases  = self.biases - np.multiply(gamma,pre_delta_b)\n",
        "                output =  self.forward_propagation(train_images)\n",
        "                g_weights,g_biases = self.back_propagation(train_images,train_labels)\n",
        "\n",
        "                delta_w = np.multiply(gamma,pre_delta_w) + np.multiply(learning_rate,g_weights)\n",
        "                delta_b = np.multiply(gamma,pre_delta_b) + np.multiply(learning_rate,g_biases)\n",
        "                \n",
        "                pre_delta_w = delta_w\n",
        "                pre_delta_b = delta_b\n",
        "                \n",
        "            if opt == 'Rmsprop': \n",
        "                 \n",
        "                vw = np.multiply(vw,beta) + np.multiply(1-beta,np.power(g_weights,2))\n",
        "                vb = np.multiply(vb,beta) + np.multiply(1-beta,np.power(g_biases,2))\n",
        "                lr_w = learning_rate/np.power(vw+eps,1/2)\n",
        "                lr_b = learning_rate/np.power(vb+eps,1/2)\n",
        "          \n",
        "                delta_w = np.multiply(g_weights,lr_w)\n",
        "                delta_b = np.multiply(g_biases,lr_b)\n",
        "\n",
        "            self.weights = self.weights - delta_w\n",
        "            self.biases  = self.biases  - delta_b\n",
        "            train_loss = -np.sum(np.multiply(train_labels,np.log(output)))/train_labels.shape[1]    \n",
        "            #print('training_loss for epoch {} = {}'.format(i,train_loss))\n",
        "\n",
        "            Validate = Model1.forward_propagation(train_val_images.T)\n",
        "            out_class=(np.argmax(Validate,axis=0))\n",
        "            target_class_validate=(np.argmax(train_val_labels,axis=1))\n",
        "            acc2 = 100*np.sum(out_class==target_class_validate)/Validate.shape[1]\n",
        "            print('Epoch {}: training_loss = {}, Validation accuracy = {}'.format(i,train_loss,acc2))\n",
        "\n",
        "\n",
        "        return train_loss"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGd-4ok12K1p",
        "outputId": "3652c578-bcd4-4501-c6e6-854fcc4bcd62"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import log_loss\n",
        "from keras.datasets import fashion_mnist\n",
        "output_classes = 10\n",
        "Model1  = Feedforwardneuralnetwork(28*28,[64,64],output_classes)\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "n_samples = train_images.shape[0]\n",
        "train_images = train_images.reshape(n_samples,-1)\n",
        "train_imag = train_images[:50000,:]\n",
        "train_val_images = train_images[50000:,:]\n",
        "labels = np.zeros((train_labels.shape[0],output_classes))\n",
        "for i in range(train_labels.shape[0]):\n",
        "  e = [0.0]*output_classes\n",
        "  e[train_labels[i]] = 1.0\n",
        "  labels[i] = e\n",
        "train_label = labels[:50000,:]\n",
        "train_val_labels = labels[50000:,:]\n",
        "mean = train_imag.mean(axis=0)\n",
        "std  = train_imag.std(axis = 0)\n",
        "train_imag = (train_imag - mean)/std\n",
        "train_val_images = (train_val_images - mean)/std\n",
        "epochs = 100\n",
        "learning_rate = 0.01\n",
        "(tr_loss) = Model1.train_model(train_imag.T,train_label.T,epochs,learning_rate,'Rmsprop')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:119: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:120: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:124: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:125: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:127: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:128: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: training_loss = 9.19240949452164, Validation accuracy = 6.51\n",
            "Epoch 1: training_loss = 5.667882941775941, Validation accuracy = 7.92\n",
            "Epoch 2: training_loss = 4.7216461303373505, Validation accuracy = 8.78\n",
            "Epoch 3: training_loss = 4.154335473588553, Validation accuracy = 12.29\n",
            "Epoch 4: training_loss = 3.8013016054403654, Validation accuracy = 14.26\n",
            "Epoch 5: training_loss = 3.5864691005891403, Validation accuracy = 16.21\n",
            "Epoch 6: training_loss = 3.413110765200276, Validation accuracy = 17.57\n",
            "Epoch 7: training_loss = 3.266358178711345, Validation accuracy = 19.39\n",
            "Epoch 8: training_loss = 3.138507156298816, Validation accuracy = 20.59\n",
            "Epoch 9: training_loss = 3.025308246887121, Validation accuracy = 21.88\n",
            "Epoch 10: training_loss = 2.9238762308663953, Validation accuracy = 22.87\n",
            "Epoch 11: training_loss = 2.832195353402546, Validation accuracy = 24.15\n",
            "Epoch 12: training_loss = 2.7487192984972295, Validation accuracy = 25.11\n",
            "Epoch 13: training_loss = 2.672293715982815, Validation accuracy = 26.28\n",
            "Epoch 14: training_loss = 2.6019707598527173, Validation accuracy = 27.23\n",
            "Epoch 15: training_loss = 2.5370087373998125, Validation accuracy = 28.27\n",
            "Epoch 16: training_loss = 2.4767752753764474, Validation accuracy = 29.07\n",
            "Epoch 17: training_loss = 2.4207573704620784, Validation accuracy = 29.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlUbQUT88KPY",
        "outputId": "f09b7c7f-c293-4c1e-a2d1-50d5bfbb72a7"
      },
      "source": [
        "\n",
        "output = Model1.forward_propagation(train_imag.T)\n",
        "out_class=(np.argmax(output,axis=0))\n",
        "target_class=(np.argmax(train_label,axis=1))\n",
        "acc2 = 100*np.sum(out_class==target_class)/output.shape[1]\n",
        "print(acc2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69.118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_WcksL736lJ"
      },
      "source": [
        "(tr_loss) = Model1.train_model(train_imag.T,train_label.T,10,learning_rate/50,'ngd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXEale9n8PAq",
        "outputId": "8077c12f-dc5a-4097-dbc0-dd3bcb196868"
      },
      "source": [
        "Validate = Model1.forward_propagation(train_val_images.T)\n",
        "out_class=(np.argmax(Validate,axis=0))\n",
        "target_class_validate=(np.argmax(train_val_labels,axis=1))\n",
        "acc2 = 100*np.sum(out_class==target_class_validate)/Validate.shape[1]\n",
        "print(acc2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68.65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8hP80WkBVxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07820890-8140-4811-9f64-618c215db301"
      },
      "source": [
        "train_imag.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65wIt9byex33"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
