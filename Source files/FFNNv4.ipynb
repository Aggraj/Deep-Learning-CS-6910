{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFNNv4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMn9NiU1QX96TDEJIKHMrzl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aggraj/Deep-Learning-CS-6910/blob/main/FFNNv4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stUyDH6Tw9D2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Feedforwardneuralnetwork:\n",
        "\n",
        "    def __init__(self,n_inputs,n_hidden,n_outputs):\n",
        "\n",
        "        self.n_inputs   = n_inputs\n",
        "        self.n_outputs  = n_outputs\n",
        "        self.n_hidden   = n_hidden\n",
        "        self.weights    = []\n",
        "        self.biases     = []\n",
        "\n",
        "        layers = [self.n_inputs] + self.n_hidden + [self.n_outputs]\n",
        "        for i in range(len(n_hidden)+1):\n",
        "            self.weights.append(np.random.randn(layers[i+1],layers[i]))\n",
        "            self.biases.append(np.random.randn(layers[i+1],1))\n",
        "\n",
        "\n",
        "\n",
        "    def sigmoid(self,x):\n",
        "        return 1 / ( 1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self,x):\n",
        "        soft = np.zeros(x.shape)\n",
        "        for i in range(0, x.shape[1]):\n",
        "            numr = np.exp(x[:, i])\n",
        "            soft[:, i] = numr/np.sum(numr)\n",
        "        return soft\n",
        "\n",
        "    def forward_propagation(self,input):\n",
        "\n",
        "        self.intermidiate_inputs = []\n",
        "        self.post_outputs  = []\n",
        "        W      = self.weights\n",
        "        b      = self.biases\n",
        "\n",
        "        k=0\n",
        "        self.intermidiate_inputs.append(np.matmul(W[k],input)+b[k])\n",
        "        self.post_outputs.append(self.sigmoid(self.intermidiate_inputs[k]))\n",
        "\n",
        "        for k in range(1,len(self.n_hidden)):\n",
        "            self.intermidiate_inputs.append(np.matmul(W[k],self.post_outputs[k-1])+b[k])\n",
        "            self.post_outputs.append(self.sigmoid(self.intermidiate_inputs[k]))\n",
        "\n",
        "        k=len(self.n_hidden)\n",
        "        self.intermidiate_inputs.append(np.matmul(W[k],self.post_outputs[k-1])+b[k])\n",
        "        self.post_outputs.append(self.softmax(self.intermidiate_inputs[k]))\n",
        "\n",
        "        return self.post_outputs[-1]\n",
        "\n",
        "    def back_propagation(self,train_images,train_labels):\n",
        "\n",
        "        g_weights = [0]*(len(self.weights))\n",
        "        g_biases  = [0]*(len(self.biases))\n",
        "        g_a       = [0]*(len(self.n_hidden)+1)\n",
        "        g_h       = [0]*(len(self.n_hidden)+1)\n",
        "        n_samples = train_images.shape[0]  # Change depending on the dimensions of data\n",
        "\n",
        "\n",
        "        for k in reversed(range(len(self.n_hidden)+1)):\n",
        "            if k == len(self.n_hidden):\n",
        "                g_a[k] = self.post_outputs[k] - train_labels  # keep or remove T depending on the dimensions of data\n",
        "                #g_weights[k] = (1/n_samples)*np.matmul(g_a[k],self.post_outputs[k-1].T)\n",
        "                #g_biases[k]  = (1/n_samples)*np.sum(g_a[k], axis=1, keepdims = True)\n",
        "            else:\n",
        "                g_h[k] = (1/n_samples)*np.matmul(self.weights[k+1].T,g_a[k+1])\n",
        "                g_a[k] = (1/n_samples)*np.multiply(g_h[k],np.multiply(self.sigmoid(self.intermidiate_inputs[k]),(1-self.sigmoid(self.intermidiate_inputs[k]))))\n",
        "\n",
        "            if k == 0:\n",
        "                g_weights[k] = (1/n_samples)*np.matmul(g_a[k],train_images.T)\n",
        "            else:\n",
        "                g_weights[k] = (1/n_samples)*np.matmul(g_a[k],self.post_outputs[k-1].T)\n",
        "\n",
        "            g_biases[k]  = (1/n_samples)*np.sum(g_a[k], axis=1, keepdims = True)\n",
        "        return g_weights,g_biases\n",
        "\n",
        "\n",
        "\n",
        "    def train_model(self,train_images,train_labels,epochs,learning_rate,opt='gd',gamma = 0.9): \n",
        "         \n",
        "        pre_delta_w = np.multiply(self.weights,0)\n",
        "        pre_delta_b = np.multiply(self.biases,0)\n",
        "        for i in range(epochs+1):\n",
        "            output =  self.forward_propagation(train_images)\n",
        "            g_weights,g_biases = self.back_propagation(train_images,train_labels)\n",
        "            if opt == 'gd':\n",
        "                delta_w = np.multiply(learning_rate,g_weights)\n",
        "                delta_b = np.multiply(learning_rate,g_biases)\n",
        "          \n",
        "            if opt == 'mgd':\n",
        "                 delta_w = np.multiply(gamma,pre_delta_w) + np.multiply(learning_rate,g_weights)\n",
        "                 delta_b = np.multiply(gamma,pre_delta_b) + np.multiply(learning_rate,g_biases)\n",
        "                 pre_delta_w = delta_w\n",
        "                 pre_delta_b = delta_b\n",
        "            \n",
        "            self.weights = self.weights - delta_w\n",
        "            self.biases  = self.biases  - delta_b\n",
        "            train_loss = -np.sum(np.multiply(train_labels,np.log(output)))/train_labels.shape[1]    \n",
        "            print('training_loss for epoch {} = {}'.format(i,train_loss))\n",
        "\n",
        "        return train_loss"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGd-4ok12K1p",
        "outputId": "ebfcf7c0-7ca3-41b7-d245-fcdca3137f8a"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import log_loss\n",
        "from keras.datasets import fashion_mnist\n",
        "output_classes = 10\n",
        "Model1  = Feedforwardneuralnetwork(28*28,[256,128],output_classes)\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "n_samples = train_images.shape[0]\n",
        "train_images = train_images.reshape(n_samples,-1)\n",
        "train_imag = train_images[:50000,:]\n",
        "train_val_images = train_images[50000:,:]\n",
        "labels = np.zeros((train_labels.shape[0],output_classes))\n",
        "for i in range(train_labels.shape[0]):\n",
        "  e = [0.0]*output_classes\n",
        "  e[train_labels[i]] = 1.0\n",
        "  labels[i] = e\n",
        "train_label = labels[:50000,:]\n",
        "train_val_labels = labels[50000:,:]\n",
        "mean = train_imag.mean(axis=0)\n",
        "std  = train_imag.std(axis = 0)\n",
        "train_imag = (train_imag - mean)/std\n",
        "train_val_images = (train_val_images - mean)/std\n",
        "epochs = 100\n",
        "learning_rate = 0.01\n",
        "(tr_loss) = Model1.train_model(train_imag.T,train_label.T,epochs,learning_rate,'mgd')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:91: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:92: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:96: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:97: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training_loss for epoch 0 = 12.52498160057375\n",
            "training_loss for epoch 1 = 9.396840609910635\n",
            "training_loss for epoch 2 = 8.884828446841588\n",
            "training_loss for epoch 3 = 9.667867905682105\n",
            "training_loss for epoch 4 = 11.043979416117114\n",
            "training_loss for epoch 5 = 8.515555168245886\n",
            "training_loss for epoch 6 = 8.10901327723656\n",
            "training_loss for epoch 7 = 7.9158423416613495\n",
            "training_loss for epoch 8 = 6.48474668426841\n",
            "training_loss for epoch 9 = 4.276613885333148\n",
            "training_loss for epoch 10 = 3.228022665032938\n",
            "training_loss for epoch 11 = 3.552669023455071\n",
            "training_loss for epoch 12 = 4.247158732233213\n",
            "training_loss for epoch 13 = 3.6566222335902387\n",
            "training_loss for epoch 14 = 2.53488601123684\n",
            "training_loss for epoch 15 = 2.9095678589369096\n",
            "training_loss for epoch 16 = 2.9007100517152122\n",
            "training_loss for epoch 17 = 2.680317512449394\n",
            "training_loss for epoch 18 = 2.491894835082479\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlUbQUT88KPY",
        "outputId": "5797cf84-3e32-46b2-8c20-9d0ace24135d"
      },
      "source": [
        "\n",
        "output = Model1.forward_propagation(train_imag.T)\n",
        "out_class=(np.argmax(output,axis=0))\n",
        "target_class=(np.argmax(train_label,axis=1))\n",
        "acc2 = 100*np.sum(out_class==target_class)/output.shape[1]\n",
        "print(acc2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45.212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_WcksL736lJ",
        "outputId": "9354d130-bc14-457f-be56-737ca00d4271"
      },
      "source": [
        "train_label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXEale9n8PAq",
        "outputId": "b25e7af1-554c-4bc5-b2f2-7af7d1cbf75e"
      },
      "source": [
        "n_inputs = 3\n",
        "n_hidden = [4,5,100]\n",
        "n_outputs = 6\n",
        "nn = Feedforwardneuralnetwork(n_inputs,n_hidden,n_outputs)\n",
        "input = np.array([[1,2,3],[2,6,2]]).T\n",
        "output= np.array([[1,5,3,1,3,3],[1,3,3,4,2,3]]).T\n",
        "A = nn.forward_propagation(input)\n",
        "B = nn.back_propagation(input,output)\n",
        "(tr_loss) = nn.train_model(input,output,epochs,learning_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training_loss for epoch 0 = 73.98759052408417\n",
            "training_loss for epoch 1 = 70.25713510358364\n",
            "training_loss for epoch 2 = 66.9700185524711\n",
            "training_loss for epoch 3 = 64.21729677564682\n",
            "training_loss for epoch 4 = 62.07692073639605\n",
            "training_loss for epoch 5 = 60.59423342999598\n",
            "training_loss for epoch 6 = 59.7657188865998\n",
            "training_loss for epoch 7 = 59.53497053881626\n",
            "training_loss for epoch 8 = 59.80534129500605\n",
            "training_loss for epoch 9 = 60.46340977911884\n",
            "training_loss for epoch 10 = 61.40128947655657\n",
            "training_loss for epoch 11 = 62.52967817473877\n",
            "training_loss for epoch 12 = 63.781293785854615\n",
            "training_loss for epoch 13 = 65.10852900321177\n",
            "training_loss for epoch 14 = 66.47907374302748\n",
            "training_loss for epoch 15 = 67.8716248504022\n",
            "training_loss for epoch 16 = 69.27245537249969\n",
            "training_loss for epoch 17 = 70.67292247119018\n",
            "training_loss for epoch 18 = 72.06774561994091\n",
            "training_loss for epoch 19 = 73.45384794060777\n",
            "training_loss for epoch 20 = 74.82958672472844\n",
            "training_loss for epoch 21 = 76.19424539951028\n",
            "training_loss for epoch 22 = 77.54769877145922\n",
            "training_loss for epoch 23 = 78.89019252235303\n",
            "training_loss for epoch 24 = 80.22219800387109\n",
            "training_loss for epoch 25 = 81.54431675967017\n",
            "training_loss for epoch 26 = 82.85721799092558\n",
            "training_loss for epoch 27 = 84.1615979178799\n",
            "training_loss for epoch 28 = 85.45815373376713\n",
            "training_loss for epoch 29 = 86.74756729798263\n",
            "training_loss for epoch 30 = 88.03049532701304\n",
            "training_loss for epoch 31 = 89.30756390770956\n",
            "training_loss for epoch 32 = 90.57936586703183\n",
            "training_loss for epoch 33 = 91.84646000754982\n",
            "training_loss for epoch 34 = 93.10937153808578\n",
            "training_loss for epoch 35 = 94.36859324563724\n",
            "training_loss for epoch 36 = 95.62458710222917\n",
            "training_loss for epoch 37 = 96.87778610106534\n",
            "training_loss for epoch 38 = 98.12859618533147\n",
            "training_loss for epoch 39 = 99.37739818030337\n",
            "training_loss for epoch 40 = 100.62454967183787\n",
            "training_loss for epoch 41 = 101.8703867965067\n",
            "training_loss for epoch 42 = 103.11522592372557\n",
            "training_loss for epoch 43 = 104.35936522042405\n",
            "training_loss for epoch 44 = 105.6030860955909\n",
            "training_loss for epoch 45 = 106.8466545264531\n",
            "training_loss for epoch 46 = 108.09032227084413\n",
            "training_loss for epoch 47 = 109.33432797199531\n",
            "training_loss for epoch 48 = 110.57889816287202\n",
            "training_loss for epoch 49 = 111.82424817757375\n",
            "training_loss for epoch 50 = 113.0705829773385\n",
            "training_loss for epoch 51 = 114.31809789850556\n",
            "training_loss for epoch 52 = 115.56697932946562\n",
            "training_loss for epoch 53 = 116.81740532320158\n",
            "training_loss for epoch 54 = 118.06954615158907\n",
            "training_loss for epoch 55 = 119.32356480712927\n",
            "training_loss for epoch 56 = 120.57961745735294\n",
            "training_loss for epoch 57 = 121.83785385666468\n",
            "training_loss for epoch 58 = 123.0984177199707\n",
            "training_loss for epoch 59 = 124.36144706204581\n",
            "training_loss for epoch 60 = 125.62707450620704\n",
            "training_loss for epoch 61 = 126.895427565529\n",
            "training_loss for epoch 62 = 128.1666288995122\n",
            "training_loss for epoch 63 = 129.44079654883149\n",
            "training_loss for epoch 64 = 130.71804415052733\n",
            "training_loss for epoch 65 = 131.99848113575834\n",
            "training_loss for epoch 66 = 133.28221291202232\n",
            "training_loss for epoch 67 = 134.56934103154845\n",
            "training_loss for epoch 68 = 135.8599633473915\n",
            "training_loss for epoch 69 = 137.15417415858624\n",
            "training_loss for epoch 70 = 138.45206434558577\n",
            "training_loss for epoch 71 = 139.75372149706456\n",
            "training_loss for epoch 72 = 141.059230029054\n",
            "training_loss for epoch 73 = 142.36867129726554\n",
            "training_loss for epoch 74 = 143.68212370336119\n",
            "training_loss for epoch 75 = 144.99966279584453\n",
            "training_loss for epoch 76 = 146.32136136616458\n",
            "training_loss for epoch 77 = 147.6472895405529\n",
            "training_loss for epoch 78 = 148.97751486805393\n",
            "training_loss for epoch 79 = 150.3121024051481\n",
            "training_loss for epoch 80 = 151.6511147973197\n",
            "training_loss for epoch 81 = 152.99461235787263\n",
            "training_loss for epoch 82 = 154.34265314425693\n",
            "training_loss for epoch 83 = 155.69529303214065\n",
            "training_loss for epoch 84 = 157.0525857874099\n",
            "training_loss for epoch 85 = 158.41458313628118\n",
            "training_loss for epoch 86 = 159.78133483365522\n",
            "training_loss for epoch 87 = 161.15288872984124\n",
            "training_loss for epoch 88 = 162.52929083575245\n",
            "training_loss for epoch 89 = 163.91058538665192\n",
            "training_loss for epoch 90 = 165.29681490452958\n",
            "training_loss for epoch 91 = 166.68802025915687\n",
            "training_loss for epoch 92 = 168.0842407278744\n",
            "training_loss for epoch 93 = 169.48551405414534\n",
            "training_loss for epoch 94 = 170.89187650490499\n",
            "training_loss for epoch 95 = 172.30336292672735\n",
            "training_loss for epoch 96 = 173.72000680082417\n",
            "training_loss for epoch 97 = 175.1418402968904\n",
            "training_loss for epoch 98 = 176.56889432579845\n",
            "training_loss for epoch 99 = 178.00119859114068\n",
            "training_loss for epoch 100 = 179.43878163962714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8hP80WkBVxM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
