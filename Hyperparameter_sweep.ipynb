{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of FFNNv7_final_sweep.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOAAapgKPHQAq29XiBUnttk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aggraj/Deep-Learning-CS-6910/blob/main/Hyperparameter_sweep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stUyDH6Tw9D2"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "class Feedforwardneuralnetwork:\n",
        "\n",
        "    def __init__(self,n_inputs,n_hidden,n_outputs,activation,loss_function):\n",
        "        self.loss_function = loss_function   \n",
        "        self.n_inputs   = n_inputs\n",
        "        self.n_outputs  = n_outputs\n",
        "        self.n_hidden   = n_hidden\n",
        "        self.activation = activation\n",
        "        self.weights    = []\n",
        "        self.biases     = []\n",
        "\n",
        "\n",
        "        layers = [self.n_inputs] + self.n_hidden + [self.n_outputs]\n",
        "        for i in range(len(n_hidden)+1):\n",
        "         #  self.weights.append(np.random.randn(layers[i+1],layers[i]))\n",
        "            self.weights = [np.random.randn(y, x)* 2/np.sqrt(x+y) for x,y in zip(layers[:-1], layers[1:])]\n",
        "            self.biases.append(np.random.randn(layers[i+1],1))\n",
        "\n",
        "\n",
        "\n",
        "    def sigmoid(self,x):\n",
        "        return 1 / ( 1 + np.exp(-x))\n",
        "\n",
        "    def tanh(self,x):\n",
        "\t      return np.tanh(x)\n",
        "       \n",
        "    def d_tanh(self,x):\n",
        "        return (1 - (np.tanh(x))**2)\n",
        "    \n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x, x)\n",
        "\n",
        "    def leaky_relu(self,x):\n",
        "       return np.where(x > 0, x, x * 0.01)\n",
        "       \n",
        "    def d_leaky_relu(self,x):\n",
        "       return np.where(x > 0, 1, 1 * 0.01) \n",
        "        \n",
        "    def d_relu(self,x):\n",
        "       return np.greater(x,0).astype(int) \n",
        "\n",
        "\n",
        "    def softmax(self,x):\n",
        "        soft = np.zeros(x.shape)\n",
        "        for i in range(0, x.shape[1]):\n",
        "            numr = np.exp(x[:, i])\n",
        "            soft[:, i] = numr/np.sum(numr)\n",
        "        return soft\n",
        "\n",
        "    def forward_propagation(self,input):\n",
        "\n",
        "        self.intermidiate_inputs = []\n",
        "        self.post_outputs  = []\n",
        "        W      = self.weights\n",
        "        b      = self.biases\n",
        "\n",
        "        k=0\n",
        "        self.intermidiate_inputs.append(np.matmul(W[k],input)+b[k])\n",
        "        if self.activation == 'sigmoid':\n",
        "          self.post_outputs.append(self.sigmoid(self.intermidiate_inputs[k]))\n",
        "        elif self.activation == 'tanh':\n",
        "          self.post_outputs.append(self.tanh(self.intermidiate_inputs[k]))\n",
        "        elif self.activation == 'relu': \n",
        "          self.post_outputs.append(self.relu(self.intermidiate_inputs[k])) \n",
        "        elif self.activation == 'leaky_relu': \n",
        "          self.post_outputs.append(self.leaky_relu(self.intermidiate_inputs[k]))\n",
        "\n",
        "        for k in range(1,len(self.n_hidden)):\n",
        "            self.intermidiate_inputs.append(np.matmul(W[k],self.post_outputs[k-1])+b[k])\n",
        "            if self.activation == 'sigmoid':\n",
        "              self.post_outputs.append(self.sigmoid(self.intermidiate_inputs[k]))\n",
        "            elif self.activation == 'tanh':\n",
        "              self.post_outputs.append(self.tanh(self.intermidiate_inputs[k]))\n",
        "            elif self.activation == 'relu': \n",
        "              self.post_outputs.append(self.relu(self.intermidiate_inputs[k])) \n",
        "            elif self.activation == 'leaky_relu': \n",
        "              self.post_outputs.append(self.leaky_relu(self.intermidiate_inputs[k]))  \n",
        "\n",
        "\n",
        "        k=len(self.n_hidden)\n",
        "        self.intermidiate_inputs.append(np.matmul(W[k],self.post_outputs[k-1])+b[k])\n",
        "        self.post_outputs.append(self.softmax(self.intermidiate_inputs[k]))\n",
        "\n",
        "        return self.post_outputs[-1]\n",
        "\n",
        "    def back_propagation(self,train_images,train_labels):\n",
        "\n",
        "        g_weights = [0]*(len(self.weights))\n",
        "        g_biases  = [0]*(len(self.biases))\n",
        "        g_a       = [0]*(len(self.n_hidden)+1)\n",
        "        g_h       = [0]*(len(self.n_hidden)+1)\n",
        "        n_samples = train_images.shape[0]  # Change depending on the dimensions of data\n",
        "\n",
        "\n",
        "        for k in reversed(range(len(self.n_hidden)+1)):\n",
        "            if k == len(self.n_hidden):\n",
        "              if self.loss_function == 'cross_entropy':\n",
        "                  g_a[k] = self.post_outputs[k]  - train_labels  # keep or remove T depending on the dimensions of data\n",
        "              elif self.loss_function == 'square_loss': \n",
        "                  g_a[k] = (self.post_outputs[k] - train_labels) * self.post_outputs[k] * (1 - self.post_outputs[k]) \n",
        "                \n",
        "            else:\n",
        "                g_h[k] = (1/n_samples)*np.matmul(self.weights[k+1].T,g_a[k+1])\n",
        "                if self.activation == 'sigmoid':\n",
        "                  g_a[k] = (1/n_samples)*np.multiply(g_h[k],np.multiply(self.sigmoid(self.intermidiate_inputs[k]),(1-self.sigmoid(self.intermidiate_inputs[k]))))\n",
        "                elif self.activation == 'tanh':\n",
        "                  g_a[k] = (1/n_samples)*np.multiply(g_h[k],self.d_tanh(self.intermidiate_inputs[k]))\n",
        "                elif self.activation == 'relu':\n",
        "                  g_a[k] = (1/n_samples)*np.multiply(g_h[k],self.d_relu(self.intermidiate_inputs[k]))\n",
        "                elif self.activation == 'leaky_relu':\n",
        "                  g_a[k] = (1/n_samples)*np.multiply(g_h[k],self.d_leaky_relu(self.intermidiate_inputs[k]))\n",
        "\n",
        "            if k == 0:\n",
        "                g_weights[k] = (1/n_samples)*np.matmul(g_a[k],train_images.T) \n",
        "            else:\n",
        "                g_weights[k] = (1/n_samples)*np.matmul(g_a[k],self.post_outputs[k-1].T)\n",
        "\n",
        "            g_biases[k]  = (1/n_samples)*np.sum(g_a[k], axis=1, keepdims = True)\n",
        "        return g_weights,g_biases\n",
        "\n",
        "\n",
        "\n",
        "    def train_model(self,train_images,train_labels,train_val_images,train_val_labels,epochs,learning_rate,opt='gd',batch_size = 32,lambd=0.0005): \n",
        "      steps = 0\n",
        "      pre_delta_w = np.multiply(self.weights,0)\n",
        "      pre_delta_b = np.multiply(self.biases,0)\n",
        "      delta_w = np.multiply(self.weights,0)\n",
        "      delta_b = np.multiply(self.biases,0)\n",
        "      vw = 0.0\n",
        "      vb = 0.0\n",
        "      eps = 1e-8\n",
        "      lr_w = 0.0\n",
        "      lr_b = 0.0\n",
        "      gamma = 0.9\n",
        "      beta = 0.999\n",
        "      beta1 = 0.9\n",
        "      beta2 = 0.999\n",
        "      m_t, v_t, m_hat_w, v_hat_w, m_b,v_b,m_hat_b,v_hat_b = 0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 \n",
        "      mu1,mu2,mu3,mu4,mu5 = 0.0,0.0,0.0,0.0,0.0\n",
        "      for i in range(epochs+1):\n",
        "        \n",
        "\n",
        "            for bb in range(0, train_images.shape[1], batch_size):\n",
        "\n",
        "              train_b_imag = train_images[:,bb:bb+batch_size]\n",
        "              train_l_imag = train_labels[:,bb:bb+batch_size]\n",
        "              output =  self.forward_propagation(train_b_imag)\n",
        "              g_weights,g_biases = self.back_propagation(train_b_imag,train_l_imag)\n",
        "              if opt == 'gd':\n",
        "                 delta_w = np.multiply(learning_rate,g_weights)\n",
        "                 delta_b = np.multiply(learning_rate,g_biases)\n",
        "          \n",
        "              if opt == 'mgd':\n",
        "                 delta_w = np.multiply(gamma,pre_delta_w) + np.multiply(learning_rate,g_weights)\n",
        "                 delta_b = np.multiply(gamma,pre_delta_b) + np.multiply(learning_rate,g_biases)\n",
        "                 pre_delta_w = delta_w\n",
        "                 pre_delta_b = delta_b\n",
        "\n",
        "              if opt == 'ngd':\n",
        "                 self.weights = self.weights - np.multiply(gamma,pre_delta_w)\n",
        "                 self.biases  = self.biases - np.multiply(gamma,pre_delta_b)\n",
        "                #output =  self.forward_propagation(train_b_imag)\n",
        "                 g_weights,g_biases = self.back_propagation(train_b_imag,train_l_imag)\n",
        "\n",
        "                 delta_w = np.multiply(gamma,pre_delta_w) + np.multiply(learning_rate,g_weights)\n",
        "                 delta_b = np.multiply(gamma,pre_delta_b) + np.multiply(learning_rate,g_biases)\n",
        "                \n",
        "                 pre_delta_w = delta_w\n",
        "                 pre_delta_b = delta_b\n",
        "                \n",
        "              if opt == 'rmsprop': \n",
        "                 \n",
        "                 vw = np.multiply(vw,beta) + np.multiply(1-beta,np.power(g_weights,2))\n",
        "                 vb = np.multiply(vb,beta) + np.multiply(1-beta,np.power(g_biases,2))\n",
        "                 lr_w = learning_rate/np.power(vw+eps,1/2)\n",
        "                 lr_b = learning_rate/np.power(vb+eps,1/2)\n",
        "          \n",
        "                 delta_w = np.multiply(g_weights,lr_w)\n",
        "                 delta_b = np.multiply(g_biases,lr_b)\n",
        "              \n",
        "              if opt == 'adam':\n",
        "                 m_t = np.multiply(beta1,m_t) + np.multiply(1-beta1,g_weights)\n",
        "                 v_t = np.multiply(beta2,v_t) + np.multiply(1-beta2,np.power(g_weights,2))\n",
        "                 m_b = np.multiply(beta1,m_b) + np.multiply(1-beta1,g_biases)\n",
        "                 v_b = np.multiply(beta2,v_b) + np.multiply(1-beta2,np.power(g_biases,2))\n",
        "                \n",
        "                 m_hat_w = m_t/(1 - np.power(beta1,i+1))\n",
        "                 m_hat_b = m_b/(1 - np.power(beta1,i+1))\n",
        "                \n",
        "                 v_hat_w = v_t/(1 - np.power(beta2,i+1))\n",
        "                 v_hat_b = v_b/(1 - np.power(beta2,i+1))\n",
        "                 delta_w = (learning_rate / np.power(v_hat_w + eps, 1/2)) * m_hat_w\n",
        "                 delta_b = (learning_rate / np.power(v_hat_b + eps, 1/2)) * m_hat_b\n",
        "              \n",
        "              if opt == 'nadam':\n",
        "                 \n",
        "                 self.weights = self.weights - np.multiply(gamma,delta_w)\n",
        "                 self.biases  = self.biases  - np.multiply(gamma,delta_b)\n",
        "                #output =  self.forward_propagation(train_b_imag)\n",
        "                 g_weights,g_biases = self.back_propagation(train_b_imag,train_l_imag)\n",
        "\n",
        "                 m_t =  np.multiply(beta1,m_t) + np.multiply(1 - beta1,g_weights)\n",
        "                 v_t =  np.multiply(beta2,v_t) + np.multiply(1 - beta2,np.power(g_weights, 2))\n",
        "\n",
        "                 m_b =  np.multiply(beta1,m_b) + np.multiply(1 - beta1,g_biases)\n",
        "                 v_b =  np.multiply(beta2,v_b) + np.multiply(1 - beta2,np.power(g_biases, 2))\n",
        "                \n",
        "                 m_hat_w = m_t / (1 - np.power(beta1, i+1)) \n",
        "                 v_hat_t = v_t / (1 - np.power(beta2, i+1))\n",
        "\n",
        "                 m_hat_b = m_b / (1 - np.power(beta1, i+1)) \n",
        "                 v_hat_b = v_b / (1 - np.power(beta2, i+1))\n",
        "  \n",
        "                 mu1 = (1-beta1)/(1-np.power(beta,i+1))\n",
        "                 mu2 = np.multiply(mu1,g_weights)\n",
        "                 mu3 = np.multiply(mu1,g_biases)\n",
        "                 \n",
        "                 mu4 = np.multiply(beta1,m_hat_w)\n",
        "                 mu5 = np.multiply(beta1,m_hat_b)\n",
        "\n",
        "                 delta_w = np.multiply(learning_rate/(np.power(v_hat_t + eps,1/2)),(mu4 + mu2))\n",
        "                 delta_b = np.multiply(learning_rate/(np.power(v_hat_b + eps,1/2)),(mu5 + mu3))\n",
        "\n",
        "             \n",
        "              self.weights = self.weights - delta_w - np.multiply(learning_rate*lambd,self.weights)\n",
        "              self.biases  = self.biases  - delta_b \n",
        "                \n",
        "            train_loss = -np.sum(np.multiply(train_l_imag,np.log(output)))/train_l_imag.shape[1]    \n",
        "            #print('training_loss for epoch {} = {}'.format(i,train_loss))\n",
        "            \n",
        "            output = self.forward_propagation(train_images)\n",
        "            out_class=(np.argmax(output,axis=0))\n",
        "            target_class=(np.argmax(train_label,axis=1))\n",
        "            acc1 = 100*np.sum(out_class==target_class)/output.shape[1]\n",
        "            \n",
        "            Validate = self.forward_propagation(train_val_images)\n",
        "            out_class=(np.argmax(Validate,axis=0))\n",
        "            target_class_validate=(np.argmax(train_val_labels,axis=1))\n",
        "            acc2 = 100*np.sum(out_class==target_class_validate)/Validate.shape[1]\n",
        "            if self.loss_function == 'cross_entropy':\n",
        "              val_loss = -np.sum(np.multiply(train_val_labels.T,np.log(Validate)))/train_val_labels.shape[0]    \n",
        "            elif self.loss_function == 'square_loss':\n",
        "              val_loss = np.sum(mean_squared_error(train_val_labels.T, Validate))\n",
        "              \n",
        " \n",
        "            print('Epoch {}: training_accuracy = {:.2f}, Validation accuracy = {:.2f}'.format(i,acc1,acc2))\n",
        "\n",
        "\n",
        "            wandb.log({\"val_accuracy\": acc2,\"accuracy\": acc1,\"steps\":epochs,\"loss\":train_loss,\"val_loss\":val_loss},)\n",
        "         \n",
        "      return acc1,acc2,train_loss,val_loss "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGd-4ok12K1p"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "\n",
        "output_classes = 10\n",
        "activation = 'relu'\n",
        "loss_function = 'cross_entropy'\n",
        "Mode  = Feedforwardneuralnetwork(28*28,[16,32],output_classes,activation,loss_function)\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "#(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "n_samples = train_images.shape[0]\n",
        "train_images = train_images.reshape(n_samples,-1)\n",
        "train_imag = train_images[:54000,:]\n",
        "train_val_images = train_images[54000:,:]\n",
        "labels = np.zeros((train_labels.shape[0],output_classes))\n",
        "for i in range(train_labels.shape[0]):\n",
        "  e = [0.0]*output_classes\n",
        "  e[train_labels[i]] = 1.0\n",
        "  labels[i] = e\n",
        "train_label = labels[:54000,:]\n",
        "train_val_labels = labels[54000:,:]\n",
        "mean = train_imag.mean(axis=0)\n",
        "std  = train_imag.std(axis = 0)\n",
        "train_imag = (train_imag - mean)/255.0\n",
        "train_val_images = (train_val_images - mean)/255.0\n",
        "#epochs = 10\n",
        "#learning_rate = 0.001\n",
        "#(tr_loss) = Mode.train_model(train_imag.T,train_label.T,train_val_images.T,train_val_labels,epochs,learning_rate,'adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHHT2kdlTPMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca84de9-2b98-4f00-c213-2b176d0a0d44"
      },
      "source": [
        "%pip install wandb -q\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRJ6zxUOtqQv"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'random', #grid, random\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [5]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-2,1e-3]\n",
        "        },\n",
        "        'opt': {\n",
        "            'values': ['nadam','rmsprop']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['relu', 'tanh']\n",
        "        },\n",
        "        'n_hidden': {\n",
        "            'values': [[16,32,64]]\n",
        "        },\n",
        "        'batch_size':{\n",
        "            'values':[32]\n",
        "        },\n",
        "        'weight_decay':{\n",
        "            'values':[0,0.0005]\n",
        "        },\n",
        "        'loss_function':{\n",
        "            'values':['cross_entropy']\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olieQYY7TQOd"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"chaxin\", project=\"Assignment 1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlUbQUT88KPY"
      },
      "source": [
        "def train():\n",
        "    steps = 0\n",
        "    # Default values for hyper-parameters we're going to sweep over\n",
        "    config_defaults = {\n",
        "        'epochs': 2,\n",
        "        'learning_rate': 1e-3,\n",
        "        'hidden':[100,200],\n",
        "        'learning_rate':1e-2,\n",
        "        'opt':'ngd',\n",
        "        'activation':'sigmoid',\n",
        "        'n_inputs': 28*28,\n",
        "        'n_outputs': 10,\n",
        "        'batch_size':100,\n",
        "        'weight_decay':0,\n",
        "        'loss_function':'cross_entropy'\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(project='Assignment 1', entity='chaxin',config=config_defaults)\n",
        "    \n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "    learning_rate = config.learning_rate\n",
        "    epochs = config.epochs\n",
        "    n_hidden = config.hidden\n",
        "    activation = config.activation\n",
        "    opt = config.opt\n",
        "    n_inputs = config.n_inputs\n",
        "    n_outputs = config.n_outputs\n",
        "    batch_size = config.batch_size\n",
        "    weight_decay = config.weight_decay\n",
        "    loss_function = config.loss_function\n",
        "    # Model training here\n",
        "    sweep_network    = Feedforwardneuralnetwork(n_inputs, n_hidden, n_outputs,activation,loss_function)\n",
        "    acc1,acc2,train_loss,val_loss  = sweep_network.train_model(train_imag.T,train_label.T,train_val_images.T,train_val_labels,epochs,learning_rate,opt,batch_size,weight_decay)\n",
        "\n",
        "#train_network(network, dataset, config.learning_rate, config.epochs, n_outputs)\n",
        "\n",
        "\n",
        "\n",
        "# 3. Log metrics over time to visualize performance\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK7Z78GHTYpE"
      },
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zx7Jj_Y9bb_"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n"
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}