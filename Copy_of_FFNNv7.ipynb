{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of FFNNv7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM5GF+vuJIhRZNtusj+HnvY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aggraj/Deep-Learning-CS-6910/blob/main/Copy_of_FFNNv7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stUyDH6Tw9D2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Feedforwardneuralnetwork:\n",
        "\n",
        "    def __init__(self,n_inputs,n_hidden,n_outputs,activation):\n",
        "\n",
        "        self.n_inputs   = n_inputs\n",
        "        self.n_outputs  = n_outputs\n",
        "        self.n_hidden   = n_hidden\n",
        "        self.activation = activation\n",
        "        self.weights    = []\n",
        "        self.biases     = []\n",
        "\n",
        "\n",
        "        layers = [self.n_inputs] + self.n_hidden + [self.n_outputs]\n",
        "        for i in range(len(n_hidden)+1):\n",
        "         #  self.weights.append(np.random.randn(layers[i+1],layers[i]))\n",
        "            self.weights = [np.random.randn(y, x)* 2/np.sqrt(x+y) for x,y in zip(layers[:-1], layers[1:])]\n",
        "            self.biases.append(np.random.randn(layers[i+1],1))\n",
        "\n",
        "\n",
        "\n",
        "    def sigmoid(self,x):\n",
        "        return 1 / ( 1 + np.exp(-x))\n",
        "\n",
        "    def tanh(self,x):\n",
        "\t      return np.tanh(x)\n",
        "       \n",
        "    def d_tanh(self,x):\n",
        "        return (1 - (np.tanh(x))**2)\n",
        "    \n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x, x)\n",
        "        \n",
        "    def d_relu(x):\n",
        "        return np.greater(x,0).astype(int)   \n",
        "\n",
        "    def softmax(self,x):\n",
        "        soft = np.zeros(x.shape)\n",
        "        for i in range(0, x.shape[1]):\n",
        "            numr = np.exp(x[:, i])\n",
        "            soft[:, i] = numr/np.sum(numr)\n",
        "        return soft\n",
        "\n",
        "    def forward_propagation(self,input):\n",
        "\n",
        "        self.intermidiate_inputs = []\n",
        "        self.post_outputs  = []\n",
        "        W      = self.weights\n",
        "        b      = self.biases\n",
        "\n",
        "        k=0\n",
        "        self.intermidiate_inputs.append(np.matmul(W[k],input)+b[k])\n",
        "        if self.activation == 'sigmoid':\n",
        "          self.post_outputs.append(self.sigmoid(self.intermidiate_inputs[k]))\n",
        "        elif self.activation == 'tanh':\n",
        "          self.post_outputs.append(self.tanh(self.intermidiate_inputs[k]))\n",
        "        elif self.activation == 'relu': \n",
        "          self.post_outputs.append(self.Relu(self.intermidiate_inputs[k])) \n",
        "\n",
        "        for k in range(1,len(self.n_hidden)):\n",
        "            self.intermidiate_inputs.append(np.matmul(W[k],self.post_outputs[k-1])+b[k])\n",
        "            if self.activation == 'sigmoid':\n",
        "              self.post_outputs.append(self.sigmoid(self.intermidiate_inputs[k]))\n",
        "            elif self.activation == 'tanh':\n",
        "              self.post_outputs.append(self.tanh(self.intermidiate_inputs[k]))\n",
        "            elif self.activation == 'relu': \n",
        "              self.post_outputs.append(self.Relu(self.intermidiate_inputs[k])) \n",
        "\n",
        "\n",
        "        k=len(self.n_hidden)\n",
        "        self.intermidiate_inputs.append(np.matmul(W[k],self.post_outputs[k-1])+b[k])\n",
        "        self.post_outputs.append(self.softmax(self.intermidiate_inputs[k]))\n",
        "\n",
        "        return self.post_outputs[-1]\n",
        "\n",
        "    def back_propagation(self,train_images,train_labels):\n",
        "\n",
        "        g_weights = [0]*(len(self.weights))\n",
        "        g_biases  = [0]*(len(self.biases))\n",
        "        g_a       = [0]*(len(self.n_hidden)+1)\n",
        "        g_h       = [0]*(len(self.n_hidden)+1)\n",
        "        n_samples = train_images.shape[0]  # Change depending on the dimensions of data\n",
        "\n",
        "\n",
        "        for k in reversed(range(len(self.n_hidden)+1)):\n",
        "            if k == len(self.n_hidden):\n",
        "                g_a[k] = self.post_outputs[k] - train_labels  # keep or remove T depending on the dimensions of data\n",
        "                #g_weights[k] = (1/n_samples)*np.matmul(g_a[k],self.post_outputs[k-1].T)\n",
        "                #g_biases[k]  = (1/n_samples)*np.sum(g_a[k], axis=1, keepdims = True)\n",
        "            else:\n",
        "                g_h[k] = (1/n_samples)*np.matmul(self.weights[k+1].T,g_a[k+1])\n",
        "                if self.activation == 'sigmoid':\n",
        "                  g_a[k] = (1/n_samples)*np.multiply(g_h[k],np.multiply(self.sigmoid(self.intermidiate_inputs[k]),(1-self.sigmoid(self.intermidiate_inputs[k]))))\n",
        "                elif self.activation == 'tanh':\n",
        "                  g_a[k] = (1/n_samples)*np.multiply(g_h[k],self.d_tanh(self.intermidiate_inputs[k]))\n",
        "                elif self.activation == 'relu':\n",
        "                  g_a[k] = (1/n_samples)*np.multiply(g_h[k],self.d_relu(self.intermidiate_inputs[k]))\n",
        "\n",
        "            if k == 0:\n",
        "                g_weights[k] = (1/n_samples)*np.matmul(g_a[k],train_images.T) \n",
        "            else:\n",
        "                g_weights[k] = (1/n_samples)*np.matmul(g_a[k],self.post_outputs[k-1].T)\n",
        "\n",
        "            g_biases[k]  = (1/n_samples)*np.sum(g_a[k], axis=1, keepdims = True)\n",
        "        return g_weights,g_biases\n",
        "\n",
        "\n",
        "\n",
        "    def train_model(self,train_images,train_labels,train_val_images,train_val_labels,epochs,learning_rate,opt='gd',batch_size = 100): \n",
        "         \n",
        "      pre_delta_w = np.multiply(self.weights,0)\n",
        "      pre_delta_b = np.multiply(self.biases,0)\n",
        "      delta_w = np.multiply(self.weights,0)\n",
        "      delta_b = np.multiply(self.biases,0)\n",
        "      vw = 0.0\n",
        "      vb = 0.0\n",
        "      eps = 1e-8\n",
        "      lr_w = 0.0\n",
        "      lr_b = 0.0\n",
        "      gamma = 0.9\n",
        "      beta = 0.999\n",
        "      beta1 = 0.9\n",
        "      beta2 = 0.999\n",
        "      m_t, v_t, m_hat_w, v_hat_w, m_b,v_b,m_hat_b,v_hat_b = 0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 \n",
        "\n",
        "      for i in range(epochs+1):\n",
        "            \n",
        "            for bb in range(0, train_images.shape[1], batch_size):\n",
        "\n",
        "              train_b_imag = train_images[:,bb:bb+batch_size]\n",
        "              train_l_imag = train_labels[:,bb:bb+batch_size]\n",
        "              output =  self.forward_propagation(train_b_imag)\n",
        "              g_weights,g_biases = self.back_propagation(train_b_imag,train_l_imag)\n",
        "              if opt == 'gd':\n",
        "                 delta_w = np.multiply(learning_rate,g_weights)\n",
        "                 delta_b = np.multiply(learning_rate,g_biases)\n",
        "          \n",
        "              if opt == 'mgd':\n",
        "                 delta_w = np.multiply(gamma,pre_delta_w) + np.multiply(learning_rate,g_weights)\n",
        "                 delta_b = np.multiply(gamma,pre_delta_b) + np.multiply(learning_rate,g_biases)\n",
        "                 pre_delta_w = delta_w\n",
        "                 pre_delta_b = delta_b\n",
        "\n",
        "              if opt == 'ngd':\n",
        "                 self.weights = self.weights - np.multiply(gamma,pre_delta_w)\n",
        "                 self.biases  = self.biases - np.multiply(gamma,pre_delta_b)\n",
        "                #output =  self.forward_propagation(train_b_imag)\n",
        "                 g_weights,g_biases = self.back_propagation(train_b_imag,train_l_imag)\n",
        "\n",
        "                 delta_w = np.multiply(gamma,pre_delta_w) + np.multiply(learning_rate,g_weights)\n",
        "                 delta_b = np.multiply(gamma,pre_delta_b) + np.multiply(learning_rate,g_biases)\n",
        "                \n",
        "                 pre_delta_w = delta_w\n",
        "                 pre_delta_b = delta_b\n",
        "                \n",
        "              if opt == 'rmsprop': \n",
        "                 \n",
        "                 vw = np.multiply(vw,beta) + np.multiply(1-beta,np.power(g_weights,2))\n",
        "                 vb = np.multiply(vb,beta) + np.multiply(1-beta,np.power(g_biases,2))\n",
        "                 lr_w = learning_rate/np.power(vw+eps,1/2)\n",
        "                 lr_b = learning_rate/np.power(vb+eps,1/2)\n",
        "          \n",
        "                 delta_w = np.multiply(g_weights,lr_w)\n",
        "                 delta_b = np.multiply(g_biases,lr_b)\n",
        "              \n",
        "              if opt == 'adam':\n",
        "                 m_t = np.multiply(beta1,m_t) + np.multiply((1-beta1),self.weights)\n",
        "                 v_t = np.multiply(beta2,v_t) + np.multiply((1-beta2),np.power(self.weights,2))\n",
        "                 m_b = np.multiply(beta1,m_b) + np.multiply((1-beta1),self.biases)\n",
        "                 v_b = np.multiply(beta2,v_b) + np.multiply((1-beta2),np.power(self.biases,2))\n",
        "                \n",
        "                 m_hat_w = m_t/(1 - np.power(beta1,i+1))\n",
        "                 m_hat_b = m_b/(1 - np.power(beta1,i+1))\n",
        "                \n",
        "                 v_hat_w = v_t/(1 - np.power(beta2,i+1))\n",
        "                 v_hat_b = v_b/(1 - np.power(beta2,i+1))\n",
        "                 delta_w = (learning_rate / np.power(v_hat_w + eps, 1/2)) * m_hat_w\n",
        "                 delta_b = (learning_rate / np.power(v_hat_b + eps, 1/2)) * m_hat_b\n",
        "              if opt == 'nadam':\n",
        "\n",
        "                 g_weights,g_biases = self.back_propagation(train_b_imag,train_l_imag)\n",
        "                 m_t = np.multiply(beta1, m_t) + np.multiply((1 - beta1),g_weights)\n",
        "                 v_t =  np.multiply(beta2,v_t) +  np.multiply((1 - beta2),np.power(g_weights, 2))\n",
        "\n",
        "                 m_b =  np.multiply(beta1,m_b) + np.multiply((1 - beta1),g_biases)\n",
        "                 v_b = np.multiply(beta2,v_b) + np.multiply((1 - beta2),np.power(g_biases, 2))\n",
        "                \n",
        "                 m_hat_t = m_t / (1 - np.power(beta1, i+1)) + np.multiply((1 - beta1),g_weights / (1 - np.power(beta1, i+1)))\n",
        "                 v_hat_t = v_t / (1 - np.power(beta2, i+1))\n",
        "\n",
        "                 m_hat_b = m_b / (1 - np.power(beta1, i+1)) + np.multiply((1 - beta1),g_biases / (1 - np.power(beta1, i+1)))\n",
        "                 v_hat_b = v_b / (1 - np.power(beta2, i+1))\n",
        "\n",
        "                 delta_w =  np.multiply(learning_rate,m_hat_w / (np.power(v_hat_w + eps,1/2)))\n",
        "                 delta_b =  np.multiply(learning_rate,m_hat_b / (np.power(v_hat_b + eps,1/2)))\n",
        "\n",
        "             \n",
        "              self.weights = self.weights - delta_w\n",
        "              self.biases  = self.biases  - delta_b\n",
        "                \n",
        "              train_loss = -np.sum(np.multiply(train_l_imag,np.log(output)))/train_l_imag.shape[1]    \n",
        "            #print('training_loss for epoch {} = {}'.format(i,train_loss))\n",
        "            \n",
        "            output = self.forward_propagation(train_images)\n",
        "            out_class=(np.argmax(output,axis=0))\n",
        "            target_class=(np.argmax(train_label,axis=1))\n",
        "            acc1 = 100*np.sum(out_class==target_class)/output.shape[1]\n",
        "            \n",
        "            Validate = self.forward_propagation(train_val_images)\n",
        "            out_class=(np.argmax(Validate,axis=0))\n",
        "            target_class_validate=(np.argmax(train_val_labels,axis=1))\n",
        "            acc2 = 100*np.sum(out_class==target_class_validate)/Validate.shape[1]\n",
        "            print('Epoch {}: training_accuracy = {:.2f}, Validation accuracy = {:.2f}'.format(i,acc1,acc2))\n",
        "\n",
        "\n",
        "      return acc2"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGd-4ok12K1p"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import log_loss\n",
        "from keras.datasets import fashion_mnist\n",
        "output_classes = 10\n",
        "activation = 'tanh'\n",
        "Mode  = Feedforwardneuralnetwork(28*28,[150,200],output_classes,'tanh')\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "n_samples = train_images.shape[0]\n",
        "train_images = train_images.reshape(n_samples,-1)\n",
        "train_imag = train_images[:54000,:]\n",
        "train_val_images = train_images[54000:,:]\n",
        "labels = np.zeros((train_labels.shape[0],output_classes))\n",
        "for i in range(train_labels.shape[0]):\n",
        "  e = [0.0]*output_classes\n",
        "  e[train_labels[i]] = 1.0\n",
        "  labels[i] = e\n",
        "train_label = labels[:54000,:]\n",
        "train_val_labels = labels[54000:,:]\n",
        "mean = train_imag.mean(axis=0)\n",
        "std  = train_imag.std(axis = 0)\n",
        "train_imag = (train_imag - mean)/std\n",
        "train_val_images = (train_val_images - mean)/std\n",
        "#epochs = 10\n",
        "#learning_rate = 0.01\n",
        "#(tr_loss) = Mode.train_model(train_imag.T,train_label.T,train_val_images.T,train_val_labels,epochs,learning_rate,'rmsprop')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHHT2kdlTPMc",
        "outputId": "b3b9209b-a721-43db-d251-3fa82021b49b"
      },
      "source": [
        "%pip install wandb -q\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRJ6zxUOtqQv"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'random', #grid, random\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [5,10,15]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-2, 1e-3, 1e-4]\n",
        "        },\n",
        "        'opt': {\n",
        "            'values': ['adam','nadam','gd', 'rmsprop','mgd','ngd']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['relu', 'tanh', 'sigmoid']\n",
        "        },\n",
        "        'n_hidden': {\n",
        "            'values': [ [32,64],[32,64,128],[100,150],[32,64,128,256]]\n",
        "        },\n",
        "        'batch_size':{\n",
        "            'values':[16,32,64,128]\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olieQYY7TQOd",
        "outputId": "386231f6-acae-4c9d-983b-860200eb8771"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"chaxin\", project=\"Assignment 1\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: ucs3kdeq\n",
            "Sweep URL: https://wandb.ai/chaxin/Assignment%201/sweeps/ucs3kdeq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlUbQUT88KPY"
      },
      "source": [
        "def train():\n",
        "    # Default values for hyper-parameters we're going to sweep over\n",
        "    config_defaults = {\n",
        "        'epochs': 2,\n",
        "        'learning_rate': 1e-3,\n",
        "        'hidden':[100,200],\n",
        "        'learning_rate':1e-2,\n",
        "        'opt':'ngd',\n",
        "        'activation':'sigmoid',\n",
        "        'n_inputs': 28*28,\n",
        "        'n_outputs': 10,\n",
        "        'batch_size':100\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(project='Assignment 1', entity='chaxin',config=config_defaults)\n",
        "    \n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "    learning_rate = config.learning_rate\n",
        "    epochs = config.epochs\n",
        "    n_hidden = config.hidden\n",
        "    activation = config.activation\n",
        "    opt = config.opt\n",
        "    n_inputs = config.n_inputs\n",
        "    n_outputs = config.n_outputs\n",
        "    batch_size = config.batch_size\n",
        "    # Model training here\n",
        "    sweep_network    = Feedforwardneuralnetwork(n_inputs, n_hidden, n_outputs,activation)\n",
        "    accuracy = sweep_network.train_model(train_imag.T,train_label.T,train_val_images.T,train_val_labels,epochs,learning_rate,opt,batch_size)\n",
        "\n",
        "#train_network(network, dataset, config.learning_rate, config.epochs, n_outputs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 3. Log metrics over time to visualize performance\n",
        "    wandb.log({\"accuracy\": accuracy})"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "gK7Z78GHTYpE",
        "outputId": "bee4889d-132c-4c50-e1c9-7a9c385e501a"
      },
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i2bnhtd1 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_hidden: [32, 64, 128]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \topt: ngd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">devout-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/chaxin/Assignment%201\" target=\"_blank\">https://wandb.ai/chaxin/Assignment%201</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/chaxin/Assignment%201/sweeps/ucs3kdeq\" target=\"_blank\">https://wandb.ai/chaxin/Assignment%201/sweeps/ucs3kdeq</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/chaxin/Assignment%201/runs/i2bnhtd1\" target=\"_blank\">https://wandb.ai/chaxin/Assignment%201/runs/i2bnhtd1</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210317_230747-i2bnhtd1</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:112: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:113: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:114: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:115: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:146: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:147: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:151: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:152: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: training_accuracy = 9.93, Validation accuracy = 9.95\n",
            "Epoch 1: training_accuracy = 7.23, Validation accuracy = 7.88\n",
            "Epoch 2: training_accuracy = 5.43, Validation accuracy = 5.87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_WcksL736lJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}