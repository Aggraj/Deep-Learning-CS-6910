{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFNNv2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNF5yzTq0M1SrXqxzttTBRu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aggraj/Deep-Learning-CS-6910/blob/main/FFNNv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stUyDH6Tw9D2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Feedforwardneuralnetwork:\n",
        "\n",
        "    def __init__(self,n_inputs,n_hidden,n_outputs):\n",
        "\n",
        "        self.n_inputs   = n_inputs\n",
        "        self.n_outputs  = n_outputs\n",
        "        self.n_hidden   = n_hidden\n",
        "        self.weights    = []\n",
        "        self.biases     = []\n",
        "\n",
        "        layers = [self.n_inputs] + self.n_hidden + [self.n_outputs]\n",
        "        for i in range(len(n_hidden)+1):\n",
        "            self.weights.append(np.random.randn(layers[i+1],layers[i]))\n",
        "            self.biases.append(np.random.randn(layers[i+1],1))\n",
        "\n",
        "\n",
        "\n",
        "    def sigmoid(self,x):\n",
        "        return 1 / ( 1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self,x):\n",
        "        soft = np.zeros(x.shape)\n",
        "        for i in range(0, x.shape[1]):\n",
        "            numr = np.exp(x[:, i])\n",
        "            soft[:, i] = numr/np.sum(numr)\n",
        "        return soft\n",
        "\n",
        "    def forward_propagation(self,input):\n",
        "\n",
        "        self.intermidiate_inputs = []\n",
        "        self.post_outputs  = []\n",
        "        W      = self.weights\n",
        "        b      = self.biases\n",
        "\n",
        "        k=0\n",
        "        self.intermidiate_inputs.append(np.matmul(W[k],input)+b[k])\n",
        "        self.post_outputs.append(self.sigmoid(self.intermidiate_inputs[k]))\n",
        "\n",
        "        for k in range(1,len(self.n_hidden)):\n",
        "            self.intermidiate_inputs.append(np.matmul(W[k],self.post_outputs[k-1])+b[k])\n",
        "            self.post_outputs.append(self.sigmoid(self.intermidiate_inputs[k]))\n",
        "\n",
        "        k=len(self.n_hidden)\n",
        "        self.intermidiate_inputs.append(np.matmul(W[k],self.post_outputs[k-1])+b[k])\n",
        "        self.post_outputs.append(self.softmax(self.intermidiate_inputs[k]))\n",
        "\n",
        "        return self.post_outputs[-1]\n",
        "\n",
        "    def back_propagation(self,train_images,train_labels):\n",
        "\n",
        "        g_weights = [0]*(len(self.weights))\n",
        "        g_biases  = [0]*(len(self.biases))\n",
        "        g_a       = [0]*(len(self.n_hidden)+1)\n",
        "        g_h       = [0]*(len(self.n_hidden)+1)\n",
        "        n_samples = train_images.shape[0]  # Change depending on the dimensions of data\n",
        "\n",
        "\n",
        "        for k in reversed(range(len(self.n_hidden)+1)):\n",
        "            if k == len(n_hidden):\n",
        "                g_a[k] = self.post_outputs[k] - train_labels  # keep or remove T depending on the dimensions of data\n",
        "                #g_weights[k] = (1/n_samples)*np.matmul(g_a[k],self.post_outputs[k-1].T)\n",
        "                #g_biases[k]  = (1/n_samples)*np.sum(g_a[k], axis=1, keepdims = True)\n",
        "            else:\n",
        "                g_h[k] = (1/n_samples)*np.matmul(self.weights[k+1].T,g_a[k+1])\n",
        "                g_a[k] = (1/n_samples)*np.multiply(g_h[k],np.multiply(self.sigmoid(self.intermidiate_inputs[k]),(1-self.sigmoid(self.intermidiate_inputs[k]))))\n",
        "\n",
        "            if k == 0:\n",
        "                g_weights[k] = (1/n_samples)*np.matmul(g_a[k],train_images.T)\n",
        "            else:\n",
        "                g_weights[k] = (1/n_samples)*np.matmul(g_a[k],self.post_outputs[k-1].T)\n",
        "\n",
        "            g_biases[k]  = (1/n_samples)*np.sum(g_a[k], axis=1, keepdims = True)\n",
        "        return g_weights,g_biases\n",
        "\n",
        "\n",
        "\n",
        "    def train_model(self,train_images,train_labels,epochs,learning_rate,opt='gd'): \n",
        "\n",
        "        for i in range(epochs+1):\n",
        "            output =  self.forward_propagation(train_images)\n",
        "            g_weights,g_biases = self.back_propagation(train_images,train_labels)\n",
        "            if opt == 'gd':\n",
        "                self.weights = self.weights - np.multiply(learning_rate,g_weights)\n",
        "                self.biases  = self.biases  - np.multiply(learning_rate,g_biases)\n",
        "            train_loss = -np.sum(np.multiply(train_labels,np.log(output)))/train_labels.shape[1]    \n",
        "            print('training_loss for epoch {} = {}'.format(i,train_loss))\n",
        "        \n",
        "        return train_loss"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGd-4ok12K1p",
        "outputId": "06221d7a-3a2e-4313-dbeb-27ffedbbdc57"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import log_loss\n",
        "from keras.datasets import fashion_mnist\n",
        "output_classes = 10\n",
        "Model1  = Feedforwardneuralnetwork(28*28,[64,128,256],output_classes)\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "n_samples = train_images.shape[0]\n",
        "train_images = train_images.reshape(n_samples,-1)\n",
        "train_imag = train_images[:50000,:]\n",
        "train_val_images = train_images[50000:,:]\n",
        "labels = np.zeros((train_labels.shape[0],output_classes))\n",
        "for i in range(train_labels.shape[0]):\n",
        "  e = [0.0]*output_classes\n",
        "  e[train_labels[i]] = 1.0\n",
        "  labels[i] = e\n",
        "train_label = labels[:50000,:]\n",
        "train_val_labels = labels[50000:,:]\n",
        "mean = train_imag.mean(axis=0)\n",
        "std  = train_imag.std(axis = 0)\n",
        "train_imag = (train_imag - mean)/std\n",
        "train_val_images = (train_val_images - mean)/std\n",
        "epochs = 100\n",
        "learning_rate = 0.01\n",
        "(tr_loss) = Model1.train_model(train_imag.T,train_label.T,epochs,learning_rate)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training_loss for epoch 0 = 26.940490187151347\n",
            "training_loss for epoch 1 = 11.450339154731589\n",
            "training_loss for epoch 2 = 9.584607917246512\n",
            "training_loss for epoch 3 = 7.615183271916383\n",
            "training_loss for epoch 4 = 6.625714955182223\n",
            "training_loss for epoch 5 = 6.319721537072568\n",
            "training_loss for epoch 6 = 5.691783865476254\n",
            "training_loss for epoch 7 = 5.750249642220648\n",
            "training_loss for epoch 8 = 5.268955884936347\n",
            "training_loss for epoch 9 = 5.305463187237358\n",
            "training_loss for epoch 10 = 5.125898571932434\n",
            "training_loss for epoch 11 = 5.167196513759506\n",
            "training_loss for epoch 12 = 5.157324353140631\n",
            "training_loss for epoch 13 = 5.021050462122701\n",
            "training_loss for epoch 14 = 4.8253387020023455\n",
            "training_loss for epoch 15 = 4.809512494193669\n",
            "training_loss for epoch 16 = 4.789281676081639\n",
            "training_loss for epoch 17 = 4.553170562175834\n",
            "training_loss for epoch 18 = 4.417353350707237\n",
            "training_loss for epoch 19 = 4.350040902327086\n",
            "training_loss for epoch 20 = 4.404356039522339\n",
            "training_loss for epoch 21 = 4.216249010871533\n",
            "training_loss for epoch 22 = 4.091567938355194\n",
            "training_loss for epoch 23 = 4.198878584235108\n",
            "training_loss for epoch 24 = 4.213457688001073\n",
            "training_loss for epoch 25 = 3.859108167656674\n",
            "training_loss for epoch 26 = 3.868106524312941\n",
            "training_loss for epoch 27 = 3.9348870410803367\n",
            "training_loss for epoch 28 = 3.9367248189143615\n",
            "training_loss for epoch 29 = 3.6024131943128763\n",
            "training_loss for epoch 30 = 3.671375799202976\n",
            "training_loss for epoch 31 = 3.7362743826579634\n",
            "training_loss for epoch 32 = 3.830123932869893\n",
            "training_loss for epoch 33 = 3.553244097876005\n",
            "training_loss for epoch 34 = 3.5750941039140196\n",
            "training_loss for epoch 35 = 3.8208173889060766\n",
            "training_loss for epoch 36 = 3.6753560416865256\n",
            "training_loss for epoch 37 = 3.680178205717557\n",
            "training_loss for epoch 38 = 3.61378705796119\n",
            "training_loss for epoch 39 = 3.672755117436708\n",
            "training_loss for epoch 40 = 3.5251733029928123\n",
            "training_loss for epoch 41 = 3.5757992048949876\n",
            "training_loss for epoch 42 = 3.5460779357324808\n",
            "training_loss for epoch 43 = 3.547587664476424\n",
            "training_loss for epoch 44 = 3.406625100950931\n",
            "training_loss for epoch 45 = 3.474829392061829\n",
            "training_loss for epoch 46 = 3.4049921234910348\n",
            "training_loss for epoch 47 = 3.450135626876152\n",
            "training_loss for epoch 48 = 3.2961574619350826\n",
            "training_loss for epoch 49 = 3.3754648305014645\n",
            "training_loss for epoch 50 = 3.277025688237808\n",
            "training_loss for epoch 51 = 3.3571426299930316\n",
            "training_loss for epoch 52 = 3.1902795587030006\n",
            "training_loss for epoch 53 = 3.274143921867435\n",
            "training_loss for epoch 54 = 3.167664642520936\n",
            "training_loss for epoch 55 = 3.264855472943054\n",
            "training_loss for epoch 56 = 3.0924340390003926\n",
            "training_loss for epoch 57 = 3.1761682904206054\n",
            "training_loss for epoch 58 = 3.0724905655286134\n",
            "training_loss for epoch 59 = 3.172978783387618\n",
            "training_loss for epoch 60 = 3.0069757179656205\n",
            "training_loss for epoch 61 = 3.084020573066037\n",
            "training_loss for epoch 62 = 2.9915889570667744\n",
            "training_loss for epoch 63 = 3.0829323467285636\n",
            "training_loss for epoch 64 = 2.938508322046421\n",
            "training_loss for epoch 65 = 3.0000579104132368\n",
            "training_loss for epoch 66 = 2.929735015025456\n",
            "training_loss for epoch 67 = 2.999544338066026\n",
            "training_loss for epoch 68 = 2.8939614439254533\n",
            "training_loss for epoch 69 = 2.9332766301307625\n",
            "training_loss for epoch 70 = 2.891979912024067\n",
            "training_loss for epoch 71 = 2.9379729032852246\n",
            "training_loss for epoch 72 = 2.866568605224535\n",
            "training_loss for epoch 73 = 2.8954485126652965\n",
            "training_loss for epoch 74 = 2.852260359316534\n",
            "training_loss for epoch 75 = 2.896881130798791\n",
            "training_loss for epoch 76 = 2.8148208084864215\n",
            "training_loss for epoch 77 = 2.855880392924408\n",
            "training_loss for epoch 78 = 2.7865551553877577\n",
            "training_loss for epoch 79 = 2.8413721058047603\n",
            "training_loss for epoch 80 = 2.7464369860202225\n",
            "training_loss for epoch 81 = 2.797511006808205\n",
            "training_loss for epoch 82 = 2.718870173079927\n",
            "training_loss for epoch 83 = 2.7775221906954295\n",
            "training_loss for epoch 84 = 2.6820932335210377\n",
            "training_loss for epoch 85 = 2.7355885092917784\n",
            "training_loss for epoch 86 = 2.6572067198600253\n",
            "training_loss for epoch 87 = 2.715028894509204\n",
            "training_loss for epoch 88 = 2.6238205622328596\n",
            "training_loss for epoch 89 = 2.6757189423852252\n",
            "training_loss for epoch 90 = 2.601176730610508\n",
            "training_loss for epoch 91 = 2.6557338310667773\n",
            "training_loss for epoch 92 = 2.570702072237225\n",
            "training_loss for epoch 93 = 2.6190732220194866\n",
            "training_loss for epoch 94 = 2.549900392070128\n",
            "training_loss for epoch 95 = 2.5999700310101694\n",
            "training_loss for epoch 96 = 2.5219696761292996\n",
            "training_loss for epoch 97 = 2.5659065094644267\n",
            "training_loss for epoch 98 = 2.5027377722561\n",
            "training_loss for epoch 99 = 2.5477611619542815\n",
            "training_loss for epoch 100 = 2.477069173440378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqoD1QWBEd57",
        "outputId": "baa421ce-190c-4f37-ed04-d4d790f50615"
      },
      "source": [
        "(tr_loss) = Model1.train_model(train_imag.T,train_label.T,10,learning_rate/100)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training_loss for epoch 0 = 2.060159480208558\n",
            "training_loss for epoch 1 = 2.060007723872062\n",
            "training_loss for epoch 2 = 2.059859372473698\n",
            "training_loss for epoch 3 = 2.0597142776142645\n",
            "training_loss for epoch 4 = 2.0595722973661115\n",
            "training_loss for epoch 5 = 2.0594332959978385\n",
            "training_loss for epoch 6 = 2.0592971437100642\n",
            "training_loss for epoch 7 = 2.059163716381822\n",
            "training_loss for epoch 8 = 2.059032895327263\n",
            "training_loss for epoch 9 = 2.058904567062235\n",
            "training_loss for epoch 10 = 2.0587786230804235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlUbQUT88KPY",
        "outputId": "7a0284b4-b171-4c3f-fad0-70e81dc6804b"
      },
      "source": [
        "\n",
        "output = Model1.forward_propagation(train_imag.T)\n",
        "out_class=(np.argmax(output,axis=0))\n",
        "target_class=(np.argmax(train_label,axis=1))\n",
        "acc2 = 100*np.sum(out_class==target_class)/output.shape[1]\n",
        "print(acc2)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52.126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_WcksL736lJ",
        "outputId": "9354d130-bc14-457f-be56-737ca00d4271"
      },
      "source": [
        "train_label.shape"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXEale9n8PAq",
        "outputId": "b25e7af1-554c-4bc5-b2f2-7af7d1cbf75e"
      },
      "source": [
        "n_inputs = 3\n",
        "n_hidden = [4,5,100]\n",
        "n_outputs = 6\n",
        "nn = Feedforwardneuralnetwork(n_inputs,n_hidden,n_outputs)\n",
        "input = np.array([[1,2,3],[2,6,2]]).T\n",
        "output= np.array([[1,5,3,1,3,3],[1,3,3,4,2,3]]).T\n",
        "A = nn.forward_propagation(input)\n",
        "B = nn.back_propagation(input,output)\n",
        "(tr_loss) = nn.train_model(input,output,epochs,learning_rate)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training_loss for epoch 0 = 73.98759052408417\n",
            "training_loss for epoch 1 = 70.25713510358364\n",
            "training_loss for epoch 2 = 66.9700185524711\n",
            "training_loss for epoch 3 = 64.21729677564682\n",
            "training_loss for epoch 4 = 62.07692073639605\n",
            "training_loss for epoch 5 = 60.59423342999598\n",
            "training_loss for epoch 6 = 59.7657188865998\n",
            "training_loss for epoch 7 = 59.53497053881626\n",
            "training_loss for epoch 8 = 59.80534129500605\n",
            "training_loss for epoch 9 = 60.46340977911884\n",
            "training_loss for epoch 10 = 61.40128947655657\n",
            "training_loss for epoch 11 = 62.52967817473877\n",
            "training_loss for epoch 12 = 63.781293785854615\n",
            "training_loss for epoch 13 = 65.10852900321177\n",
            "training_loss for epoch 14 = 66.47907374302748\n",
            "training_loss for epoch 15 = 67.8716248504022\n",
            "training_loss for epoch 16 = 69.27245537249969\n",
            "training_loss for epoch 17 = 70.67292247119018\n",
            "training_loss for epoch 18 = 72.06774561994091\n",
            "training_loss for epoch 19 = 73.45384794060777\n",
            "training_loss for epoch 20 = 74.82958672472844\n",
            "training_loss for epoch 21 = 76.19424539951028\n",
            "training_loss for epoch 22 = 77.54769877145922\n",
            "training_loss for epoch 23 = 78.89019252235303\n",
            "training_loss for epoch 24 = 80.22219800387109\n",
            "training_loss for epoch 25 = 81.54431675967017\n",
            "training_loss for epoch 26 = 82.85721799092558\n",
            "training_loss for epoch 27 = 84.1615979178799\n",
            "training_loss for epoch 28 = 85.45815373376713\n",
            "training_loss for epoch 29 = 86.74756729798263\n",
            "training_loss for epoch 30 = 88.03049532701304\n",
            "training_loss for epoch 31 = 89.30756390770956\n",
            "training_loss for epoch 32 = 90.57936586703183\n",
            "training_loss for epoch 33 = 91.84646000754982\n",
            "training_loss for epoch 34 = 93.10937153808578\n",
            "training_loss for epoch 35 = 94.36859324563724\n",
            "training_loss for epoch 36 = 95.62458710222917\n",
            "training_loss for epoch 37 = 96.87778610106534\n",
            "training_loss for epoch 38 = 98.12859618533147\n",
            "training_loss for epoch 39 = 99.37739818030337\n",
            "training_loss for epoch 40 = 100.62454967183787\n",
            "training_loss for epoch 41 = 101.8703867965067\n",
            "training_loss for epoch 42 = 103.11522592372557\n",
            "training_loss for epoch 43 = 104.35936522042405\n",
            "training_loss for epoch 44 = 105.6030860955909\n",
            "training_loss for epoch 45 = 106.8466545264531\n",
            "training_loss for epoch 46 = 108.09032227084413\n",
            "training_loss for epoch 47 = 109.33432797199531\n",
            "training_loss for epoch 48 = 110.57889816287202\n",
            "training_loss for epoch 49 = 111.82424817757375\n",
            "training_loss for epoch 50 = 113.0705829773385\n",
            "training_loss for epoch 51 = 114.31809789850556\n",
            "training_loss for epoch 52 = 115.56697932946562\n",
            "training_loss for epoch 53 = 116.81740532320158\n",
            "training_loss for epoch 54 = 118.06954615158907\n",
            "training_loss for epoch 55 = 119.32356480712927\n",
            "training_loss for epoch 56 = 120.57961745735294\n",
            "training_loss for epoch 57 = 121.83785385666468\n",
            "training_loss for epoch 58 = 123.0984177199707\n",
            "training_loss for epoch 59 = 124.36144706204581\n",
            "training_loss for epoch 60 = 125.62707450620704\n",
            "training_loss for epoch 61 = 126.895427565529\n",
            "training_loss for epoch 62 = 128.1666288995122\n",
            "training_loss for epoch 63 = 129.44079654883149\n",
            "training_loss for epoch 64 = 130.71804415052733\n",
            "training_loss for epoch 65 = 131.99848113575834\n",
            "training_loss for epoch 66 = 133.28221291202232\n",
            "training_loss for epoch 67 = 134.56934103154845\n",
            "training_loss for epoch 68 = 135.8599633473915\n",
            "training_loss for epoch 69 = 137.15417415858624\n",
            "training_loss for epoch 70 = 138.45206434558577\n",
            "training_loss for epoch 71 = 139.75372149706456\n",
            "training_loss for epoch 72 = 141.059230029054\n",
            "training_loss for epoch 73 = 142.36867129726554\n",
            "training_loss for epoch 74 = 143.68212370336119\n",
            "training_loss for epoch 75 = 144.99966279584453\n",
            "training_loss for epoch 76 = 146.32136136616458\n",
            "training_loss for epoch 77 = 147.6472895405529\n",
            "training_loss for epoch 78 = 148.97751486805393\n",
            "training_loss for epoch 79 = 150.3121024051481\n",
            "training_loss for epoch 80 = 151.6511147973197\n",
            "training_loss for epoch 81 = 152.99461235787263\n",
            "training_loss for epoch 82 = 154.34265314425693\n",
            "training_loss for epoch 83 = 155.69529303214065\n",
            "training_loss for epoch 84 = 157.0525857874099\n",
            "training_loss for epoch 85 = 158.41458313628118\n",
            "training_loss for epoch 86 = 159.78133483365522\n",
            "training_loss for epoch 87 = 161.15288872984124\n",
            "training_loss for epoch 88 = 162.52929083575245\n",
            "training_loss for epoch 89 = 163.91058538665192\n",
            "training_loss for epoch 90 = 165.29681490452958\n",
            "training_loss for epoch 91 = 166.68802025915687\n",
            "training_loss for epoch 92 = 168.0842407278744\n",
            "training_loss for epoch 93 = 169.48551405414534\n",
            "training_loss for epoch 94 = 170.89187650490499\n",
            "training_loss for epoch 95 = 172.30336292672735\n",
            "training_loss for epoch 96 = 173.72000680082417\n",
            "training_loss for epoch 97 = 175.1418402968904\n",
            "training_loss for epoch 98 = 176.56889432579845\n",
            "training_loss for epoch 99 = 178.00119859114068\n",
            "training_loss for epoch 100 = 179.43878163962714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8hP80WkBVxM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}